---
title: "dynamic_eqtl"
author: "Lifan Liang"
date: "2025-01-06"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Model formulation

Suppose we have expression data of $n$ samples from $D$ donors. Here a sample could be a single cell, or a meta-cell, or some pseudobulk sample (of many cells) with the same type/state. For each sample, we denote $X_i$ as the context of interest, e.g. treatment, cell state (such as differentiation pseudotime). Additionally, genotype $G_d, 1 \leq d \leq D$ may also affect expression. There is also a possible interaction effect between genotype $G_d$ and the context $X_i$. Our model of gene expression is: 
\begin{equation}
    y_i = \mu + \beta_X X_i + \beta_G G_d + \gamma X_i \cdot G_d + u_d + \epsilon_i,
\end{equation}
where $\beta_X$ and $\beta_G$ are context, and genotype effects, respectively, $\gamma$ is the interaction effect, and $u_d$ is the donor-specific random effect. For simplicity, we do not include some covariates such as sex and age of donors, and percent of mitochondrial reads of cells. 

Fitting the model is computationally expensive. We can rewrite the model to remove the donor random effect. We remove from $y_i$, the average expression of all samples from the donor $d$, $\bar{y}_d$:  
\begin{equation}
    \Delta_i = y_i - \bar{y}_d. 
\end{equation}
Now we can write our model as: 
\begin{equation}
    \Delta_i = \beta_X X_i + \gamma X_i G_d + \epsilon_i = X_i (\beta_X + \gamma G_d) + \epsilon_i. 
\end{equation}
From this model, we can see that the effect of $X_i$ on gene expression depends on the genotype $G_d$, denoted as
\begin{equation}
    \theta_d = \beta_X + \gamma G_d
\end{equation}
This can be viewed as: ``genes modify the environmental effect``. 


## Model fitting

The simplest way to solve the model is a two-step linear regression. We first regress environment $X$ against $\Delta_i$ to estimate the total effect for each donor, $\theta_d$. And the corresponding standard error $\sigma_d$. 

$$
\hat{\theta_d} = (X_d^TX_d)^{-1}X_d^T \Delta_d 
$$

$$
\sigma_d^2 = var(\hat{\theta_d}) = \dfrac{var(Y_d)}{X_d^T X_d}
$$

where $X_d$ is the environment variable in the treatment effect.

Then we find the MLE for \gamma:

$$
\hat{\theta}_d \sim Normal(\beta_X + \gamma G_d, \sigma_d^2)
$$

Let us denote $1/\sigma_d^2$ as $W_d$, the objective function to optimize is:

$$
\sum_d^D{W_d(\hat{\theta}_d - \beta_X - \gamma G)^2}
$$

If we centered the mean of $\hat{\theta}_d$ (weighted by $W$) and suppose $G$ is standardized (mean zero in single cell level, equivalent to $\bar G=0$ in the next section), then $\beta_X=0$. The estimate of $\gamma$ is:

$$
\hat{\gamma} = \dfrac{\sum W_d \theta'_d G_d}{\sum W_d G_d^2}
\\
\theta'_d = \hat \theta_d - \bar \theta
\\
\bar \theta = \dfrac{\sum W_d \hat{\theta_d}}{\sum W_d}
$$

As for the standard error of $\hat \gamma$, we have

$$
var(\hat \gamma) = \dfrac{1}{\sum W_dG_d^2} = \dfrac{var(Y)}{\sum G_i^2 X_i^2}
$$




## Equivalence to the cell level interaction model

Alternative to the two-step model above, we can directly fit an interaction model on single-cell level:

$$
\Delta_i = \beta_x X + \gamma X\cdot G
$$

First of all, the weighted average of $\hat{\theta_d}$ is identical to MLE of $\beta_x$ given that genotype has mean zero. Intuitively, this is correct since

$$
E(\theta) = E(\beta_x+\gamma G) = \beta_x + \gamma E(G)
$$

More formally, we first derive estimator of $\bar \theta$ and $\bar G$ on cell level. Based on inverse variance weighted (IVW) meta-analysis or direct derivation, the weighted average of $\hat{\theta}$ is

$$
\bar \theta = \dfrac{\sum W_d \theta_d}{\sum W_d} = \dfrac{X^T \Delta}{X^T X}
$$

Similarly, the weighted average of $G$ is

$$
\bar G = \dfrac{\sum W_d G}{\sum W_d} = \dfrac{X^T G' X}{X^T X} = \dfrac{\sum G_i X_i^2}{\sum X_i^2}
$$

where $G'$ is a $n\times n$ diagonal matrix with the diagonal elements as the genotypes

Since the log Likelihood for the interaction model is:

$$
\mathcal{L}(\Delta;\beta_x,\gamma) \propto -\sum(\Delta_i-\beta_xX_i-\gamma G_i\cdot X_i)^2
$$

By setting the $\dfrac{d\mathcal{L}(\Delta;\beta_x,\gamma)}{d\beta_x}=0$ to 0, we should find the MLE for $\beta_x$ as:

$$
\hat \beta_x = \dfrac{\sum\Delta_iX_i}{\sum X_i^2} - \gamma \dfrac{\sum G_i X_i^2}{\sum X_i^2} = \bar \theta - \gamma \bar G
$$

Therefore, by setting $\bar G = 0$, the estimator for $\hat \beta_x$ is identical to that of $\bar \theta$. 

Similarly, we can derive the MLE of $\gamma$. Let us denote the interaction term $X\cdot G$ as $X'$ for convenience.

$$
\hat \gamma = \dfrac{X'^T \Delta}{X'^TX'} - \hat \beta \dfrac{X^TX'}{X'^TX'}
$$

Recall that the estimate for $\gamma$ in the two step model is:

$$
\hat \gamma = \dfrac{\sum W_d (\hat \theta_d-\bar \theta) G}{\sum W_d G^2} = \dfrac{X'^T \Delta}{X'^TX'} - \hat \beta \dfrac{X^TX'}{X'^TX'}
$$

Therefore, $\hat \gamma$ is identical in the two approaches.

As for the standard error of $\hat \gamma$, it is obvious that they are the same by plugging in the estimate of $W_d$ in previous equation for $var(\hat \gamma)$.

The advantage of using a two-step model instead of the interaction model is that we decoupled the genetic variation and phenotypic variation with an intermediate "trait" that captures the sufficient statistics for each donor. Hence, when we need to run QTL mapping for the same molecular trait, we only need to repeat the second step on bulk level while retaining the power of single-cell level model. This would bring enormous speed up to the dynamic QTL mapping procedure.

## Simulation

For loop is required when computing $\theta$ for each donor. This incurs extra computational cost in R and python and it leads to slower speed compared the full interaction model. I decided to implement the two-step model in Julia and used the highly optimized package, `MixedModels`, to construct the baseline mixed effect model for a fair comparison.

Data generating process follows:

$$
G_d \sim Multnominal()
\\
E_i \sim Uniform(0,1)
\\
U_d \sim Normal(0,\sigma_U^2)
\\
\epsilon \sim Normal(0,\sigma^2)
\\
Y_{id} = \alpha G_d + \beta E_i + \phi G\cdot E + U_d + \epsilon
$$

G is random selected from $\{0,1,2\}$ with equal chance, $\alpha=0.05$, $\beta=0.1$, $\sigma_U^2=0.2$, $\epsilon=1.0$. If we compute PVE as $\dfrac{\alpha VAR(G)}{VAR(Y)}$, it would be around 3.5%.

### Effect estimation and power

We repeated the simulation 500 times for different $\phi$ (i.e, 0.02, 0.04, 0.05)
The estimation of $\phi$ is quite similar between mixed effect and our two-step fixed effect model.

```{r,echo=F}
library(ggplot2)
fixed_resfs <- list.files("data/",pattern="twostep",full.names = T)
fixed_res <- lapply(fixed_resfs,read.csv)[-1]
mixed_resfs <- list.files("data/",pattern="mixedmodel",full.names = T)
mixed_res <- lapply(mixed_resfs,read.csv)[-1]

fix.beta <- do.call(c, lapply(fixed_res,function(x){x$beta}))
mix.beta <- do.call(c, lapply(mixed_res,function(x){x$beta}))
dat.beta <- data.frame(beta=c(fix.beta,mix.beta),
                       oracle=factor(rep(rep(c(0.02,0.04,0.05),each=500),2)),
                       model=rep(c("fixed_effect","mixed_effect"),each=1500))
ggplot(dat.beta,aes(x=oracle,y=beta,fill=model)) + geom_boxplot() + theme_bw() +
  ylab("Estmated phi")
```

The power is also quite similar. Here we look into the Z score when $\phi=0.05$.

```{r,echo=F}
fix.z <- fixed_res[[3]]$beta / fixed_res[[3]]$se
mix.z <- mixed_res[[3]]$beta / mixed_res[[3]]$se
plot(fix.z, mix.z, pch=20,col=2,
     ylab="Z score from mixed effect model",
     xlab="Z score from fixed effect model")
lines(c(0,9),c(0,9),col=1)
```


### P value calibration analysis

We randomly generated 9999 null SNPs and 1 causal SNPs. Black dots represent p values with the null SNPs. The green dashed line indicates the p value from the causal SNPs. We can see both models have slightly inflated P values, which is common for dynamic eQTL. But mixed effect models cannot run permutation testing to calibrate P values due to the heavy computational cost.

```{r,echo=F}
fix_null <- read.csv("data/multiSNP_fixed_two_step_res.csv")
fix_null$Z <- fix_null$beta / fix_null$se
null.p <- sort(pt(fix_null$Z[-1],1e5-3,lower.tail = F))
plot(y=-log10(null.p),
     x=-log10(1:length(null.p)/length(null.p)),pch=20,
     xlab="expected -log10(P)", ylab="observed -log10(P)",
     main="P values from fixed effect model")
lines(c(0,5),c(0,5),col=2)
abline(h=-log10(pt(fix_null$Z[1],1e5-3,lower.tail = F)),lty=3,col=3,lwd=3)
```


```{r,echo=F}
mix_null <- read.csv("data/multiSNP_mixed_res.csv")
mix_null$Z <- mix_null$beta / mix_null$se
null.p <- sort(pt(mix_null$Z[-1],1e5-3,lower.tail = F))
plot(y=-log10(null.p),
     x=-log10(2:nrow(mix_null)/nrow(mix_null)),pch=20,
     xlab="expected -log10(P)", ylab="observed -log10(P)",
     main="P values from mixed effect")
lines(c(0,5),c(0,5),col=2)
abline(h=-log10(pt(mix_null$Z[1],1e5-3,lower.tail = F)),lty=3,col=3,lwd=3)
```

### Running time comparison

When we only run the procedure once, mixed effect model is slightly more efficient because of its optimized implementation, while our implementation is still pretty crude.

```{r,echo=F}
fix.time <- do.call(c, lapply(fixed_res,function(x){x$time}))
mix.time <- do.call(c, lapply(mixed_res,function(x){x$time}))
dat.time <- data.frame(time=c(fix.time,mix.time),
                       oracle=factor(rep(rep(c(0.02,0.04,0.05),each=500),2)),
                       model=rep(c("fixed_effect","mixed_effect"),each=1500))
ggplot(dat.time,aes(x=oracle,y=time,fill=model)) + geom_boxplot() + theme_bw() +
  ylab("Running time (unit: second)") + scale_y_log10()
```

However, if we repeat the procedure with one trait and multiple SNPs, our method scales much better than mixed effect model.

```{r,echo=F}
msnp_time = read.csv("data/scalability_snp.csv")
msnp_time = rbind(c(median(fix.time),median(mix.time)), msnp_time)
dat.mtime <- data.frame(time=c(msnp_time$mixed,msnp_time$two_step),
                        Nsnp=rep(c(1,10,100,1000,10000),2),
                        model=rep(c("mixed_effect","two_step"),each=nrow(msnp_time)))
ggplot(dat.mtime,aes(y=time,x=Nsnp,shape=model,color=model)) + geom_line() + geom_point() + 
  theme_bw() + scale_y_log10() + scale_x_log10() + ylab("running time (unit: second)")
```

Suppose we have 20K genes, each with 1000 candidate SNPs. Then our two-step method takes 5 minutes to scan all gene-snp pairs, while the mixed effect model will take 28 hours roughly. This advantage in scalability allow us to perform permutation testing to obtain calibrated P values and fine mapping to identify causal SNPs.

