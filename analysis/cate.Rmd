---
title: "More flexible estimation of GxE effect"
author: "Lifan Liang"
date: "2025-04-23"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

Following the  interaction effect model described at the early sections of [dynamic eQTL](dynamic_eqtl.html). We have:

$$
    \Delta_i = \alpha X_i + \gamma X_i G_d + \epsilon_i
$$

The assumption that $X$ has a linear effect on $y$ may not hold, especially when $X$ are not derived from factorization of $y$. For example, pseudo time or cell state are inferred by grouping similar cells together without linear decomposition. If the relationship between $X$ and $y$ are not captured correctly, the residual, $\sigma$ would be much larger. As a result, the standard error of $\beta$, for instance, computed as $\dfrac{\sigma}{var(G)}$, would be much larger. Therefore, statistical power would be severely compromised.

## Functional form of the interaction effect model

We replace the linear terms related to X with the more flexible functional form.

$$
\Delta_i = f(X_i) + G_d \cdot\tau(X_i) + \epsilon_i
$$

### Model fitting

Following the [R-learner approach](https://doi.org/10.1093/biomet/asaa076) for Conditional average treatment effect (CATE), there are two stages.

First, we fit the $f(x)$ on $\Delta$ with a machine learning method. Also, we fit the propensity function ($e(X)$) on $G$. Then we have:

$$
\tilde{\Delta} = \Delta - f(X)
$$
$$
\tilde{G} = G - e(X)
$$
This step is proposed as the double machine learning (DML) approach. The model becomes:

$$
\tilde{\Delta} = \tilde{G} \cdot \tau(X) + \epsilon
$$

To learn the function $\tau(X)$, we need to minimize the square error as the objective function:

$$
{\cal L} = \sum_i[\tilde{\Delta_i} - \tilde{G_i} \cdot \tau(X_i)]^2 = \sum_i{G_i^2}[\dfrac{\tilde{\Delta_i}}{G_i} -\tau(X_i)]^2
$$

Therefore, we regress $X$ against $\dfrac{\tilde{\Delta_i}}{G_i}$ with a certain machine learning method (e.g. XGboost) with samples weights $G_i^2$. Considering the issues of overfitting, cross-fitting seems required for this procedure. How to scale up this method to test CATE of genetic effects would be a major concern.

## Testing the heterogeneity of effect size.

After estimating $\tau(X)$, we can compute the variance of $\tau(X)$ across cells. Then we get null distribution by permuting the genotypes of the cells and estimate $VAR\tau(X)$ during each permutation. Cells with permuted genotypes are supposed to have homogeneous effects instead. Then P value would be the proportion of null values larger than the observed $VAR(\tau(X))$ in real data.

## Simulation experiment

### The causal model

$$
y = X\alpha_1 + X^2\alpha_2 + GX\phi_1 + GX^2\phi_2 + Glog(X) \phi_3 + \epsilon
$$

100 donors with 100 cells each.

```{r}
sim_cate <- function(alpha1=0.1, alpha2=0.05, phi1=0.02, phi2=0.05, phi3=0.01, eps=1){
  res <- numeric(6L)
  
  X = rnorm(10000,sd=1)
  G = rnorm(100, sd=1)
  G1 = rep(G, each=100)
  eff.cell = X*phi1 + X^2*phi2 + log(abs(X))*phi3
  y = X*alpha1 + X^2*alpha2 + G1*eff.cell + rnorm(100,sd=eps)
  list(X=X, y=y, Gd=G, Gc=G1, eff.oracle=eff.cell)
}

```


```{r}
library(xgboost) ## Fitting function with xgboost
dat <- sim_cate()
fit.x2y <- xgboost(as.matrix(dat$X),dat$y,nrounds = 10, objective="reg:linear")
y1 <- dat$y - predict(fit.x2y, as.matrix(dat$X))

tao <- xgboost(as.matrix(dat$X), y1/dat$Gc, nrounds=5, 
               objective="reg:linear", weight=dat$Gc^2)
eff.oracle <- dat$eff.oracle
eff.pred <- predict(tao, as.matrix(dat$X))
print(cor(eff.oracle, eff.pred)^2)
plot(eff.oracle, eff.pred)
```


```{r}
## Fitting function with splnes
## Somehow samples weights are normalized in lm()
x2 <- dat$X^2
x3 <- dat$X^3
spline.x2y <- lm(y~X+x2+x3, data=dat)
dat$y1 <- spline.x2y$residuals/dat$Gc
spline.tao <- lm(y1~X+x2+x3, data=dat,weights=Gc^2)
eff.oracle <- dat$eff.oracle
eff.spline <- predict(spline.tao, dat)
print(cor(eff.oracle, eff.spline)^2)
plot(eff.oracle, eff.spline, pch=20)
```


```{r}
## Linear regression fitting
fit.ols <- summary(lm(y~Gc+X+X*Gc,dat))
eff.ols <- fit.ols$coefficients[2,1] + fit.ols$coefficients[4,1] * dat$X
print(cor(eff.oracle, eff.ols)^2)
plot(eff.oracle, eff.ols, pch=20, col=2, main="Linear interaction model")
```


```{r}
## Split the cells into 6 quantiles of pseudotime and tested quadratic effects
x.bin <- numeric(length(dat$X))
for(i in 1:6) {
  lower = quantile(dat$X,(i-1)/6)
  upper = quantile(dat$X,i/6)
  x.bin[dat$X>=lower & dat$X<=upper] = i
}
dat$x.bin <- x.bin
dat$x.bin2 <- x.bin^2
fit.bin <- summary(lm(y~Gc+x.bin+Gc*x.bin+Gc*x.bin2, dat))
eff.bin <- dat$x.bin*fit.bin$coefficients[5,1] + dat$x.bin2*fit.bin$coefficients[5,2] + fit.bin$coefficients[2,1]
plot(eff.oracle, eff.bin, pch=20, col=3, main="Binned quadratic effect model")
print(cor(eff.oracle, eff.bin)^2)
```

```{r}
par(mfrow=c(1,3))
plot(eff.oracle, eff.spline, pch=20, main="R-learner", xlab="True effect", ylab="Estimated effects")
plot(eff.oracle, eff.ols, pch=20, col=2, main="Linear interaction model", xlab="True effect", ylab="Estimated effects")
plot(eff.oracle, eff.bin, pch=20, col=3, main="Binned quadratic effect model", xlab="True effect", ylab="Estimated effects")
```


```{r}
eff.all <- c(eff.bin,eff.ols,eff.spline, eff.oracle)
ylim <- c(min(eff.all),max(eff.all))
plot(dat$eff.oracle, dat$eff.oracle, pch=20, col=1, xlab="True effect", ylab="Estimated effect", ylim=ylim)
points(dat$eff.oracle, eff.bin, pch=2, col=2)
points(dat$eff.oracle, eff.spline, pch=3, col=3)
points(dat$eff.oracle, eff.ols, pch=4, col=4)
#lines(c(-1,1),c(-1,1), pch=20, col=4)
```

